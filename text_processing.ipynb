{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06579014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing in NLP\n",
    "# author: Muhammad Humayun Khan\n",
    "# The mandatory steps in text preprocessing include:\n",
    "# 1. Lowercasing\n",
    "# 2. Removing HTML tags\n",
    "# 3. Removing URLs  \n",
    "# 4. Removing punctuation\n",
    "# 5. Chat words treatment\n",
    "# 6. Spelling correction\n",
    "# 7. Removing stop words\n",
    "# 8. Handling emojis\n",
    "# 9. Tokenization\n",
    "# 10. Lemmatization\n",
    "# 11. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae657b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56857ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'datasets/IMDB Dataset.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. lowercase one of the record\n",
    "\n",
    "df['review'][2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443777f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all the records in the review column\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a43dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove the unimportant things such as html tags\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7c45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. the URL links are not important and needs to be removed\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from a string.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_urls)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89f34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Remove the punctuation marks\n",
    "# first check the punctuation list\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d391ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the punctuation marks\n",
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "956fbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a string.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eed999cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String with punctuation  '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"String with punctuation! # @\"\n",
    "\n",
    "text = remove_punctuation(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e4b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production the filming tech...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically theres a family where a little boy j...\n",
       "4        petter matteis love in the time of money is a ...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    im going to have to disagree with the previous...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now want to apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_punctuation)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3728b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Chatword treatment - short words like \"u\" for \"you\", \"r\" for \"are\", etc.\n",
    "\n",
    "chatwords = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"ur\": \"your\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"b4\": \"before\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"pls\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"smh\": \"shaking my head\",   \n",
    "    \"btw\": \"by the way\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"lmao\": \"laughing\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"wbu\": \"what about you\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idc\": \"I don't care\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"thx\": \"thanks\",    \n",
    "    \"k\": \"okay\",\n",
    "    \"plz\": \"please\",\n",
    "    \"wth\": \"what the hell\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"fml\": \"f*** my life\",\n",
    "    \"tbh\": \"to be honest\"\n",
    "}\n",
    "def chatword_treatment(text):        \n",
    "    for word, replacement in chatwords.items():\n",
    "        text = text.replace(word, replacement)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(chatword_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a0b12ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have a dream that one day this nation will rise up and live out the true meaning of its creed: 'He hold these truths to be self-e\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Spelling correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I havv a dreem that one day this nation will rise up and live out the true meaning of its creed: 'We hold these truths to be self-e\"\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Correct spelling in a string.\"\"\"\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "text = correct_spelling(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8963d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Just\n",
      "[nltk_data]     Bring\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Removing stop words - use the nltk library. Avoid stop words when your task is POS tagging \n",
    "import nltk\n",
    "nltk.download('stopwords')  # Download the stopwords dataset if not already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f567e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one otheare areevieweares mentioned afteare wa...\n",
       "1        wondearefyoul little pareodyouction filming te...\n",
       "2        thoyought wondearefyoul way spend time hot syo...\n",
       "3        basically thearees family whearee little boy j...\n",
       "4        petteare matteis love time money visyoually st...\n",
       "                               ...                        \n",
       "49995    thoyought movie areight good job wasnt careeat...\n",
       "49996    bad plot bad dialogyoue bad acting idiotic dia...\n",
       "49997    catholic tayought paareochial elementaarey sch...\n",
       "49998    im going disagareee pareevioyous comment side ...\n",
       "49999    one expects staare tareeokay movies high aaret...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords   \n",
    "stop_words = set(stopwords.words('english'))   # only english stop words. Others can be added as needed\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from a string.\"\"\"\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffa19b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Programming :red_heart:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Handling emojis - use the emoji library\n",
    "import emoji\n",
    "\n",
    "text = \"I love Programming ❤️\"\n",
    "def handle_emojis(text):\n",
    "    \"\"\"Convert emojis to text.\"\"\"\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "text = handle_emojis(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65106154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', 'and', 'natural', 'language', 'processing.', \"It's\", 'amazing!']\n",
      "[\"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is\", '']\n"
     ]
    }
   ],
   "source": [
    "# 8. Tokenization - \n",
    "# There are different techniques such as using the split function, regular expressions, use the nltk library and spacy library\n",
    "\n",
    "# split function technique\n",
    "text = \"I love programming and natural language processing. It's amazing!\"\n",
    "tokens = text.split()\n",
    "print(tokens)   # word tokenization\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "tokens_two = text_two.split('.')\n",
    "print(tokens_two)   # sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', 'and', 'natural', 'language', 'processing', 'It', 's', 'amazing']\n",
      "[\"Do you love programming and natural language processing? It's amazing\", \" Isn't it? Yes, it is\", '']\n"
     ]
    }
   ],
   "source": [
    "# regular expression technique\n",
    "import re\n",
    "\n",
    "text = \"I love programming and natural language processing. It's amazing!\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)   # word tokenization\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "tokens_two = re.split(r'[.!]', text_two)\n",
    "print(tokens_two)   # sentence tokenization and issue is ? is missing and it is not identified as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749fa837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Path being used: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\n",
      "NLTK data paths (internal): ['C:\\\\Users\\\\Just Bring/nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\share\\\\nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Just Bring\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n",
      "Attempting to download 'punkt' package...\n",
      "'punkt' package download/check complete.\n",
      "\n",
      "Attempting to download 'punkt_tab' package...\n",
      "'punkt_tab' package download/check complete.\n",
      "SUCCESS: punkt file found at: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\\tokenizers\\punkt\\english.pickle\n",
      "ERROR: Expected punkt_tab file NOT found at: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\\tokenizers\\punkt_tab\\english.pickle\n",
      "Please check the contents of your venv/nltk_data folder manually.\n",
      "It should contain 'tokenizers/punkt_tab/english.pickle'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to d:\\Drive I\\Work\\Summer 2025\n",
      "[nltk_data]     Personal\n",
      "[nltk_data]     Growth\\natural_language_processing\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to d:\\Drive I\\Work\\Summer\n",
      "[nltk_data]     2025 Personal\n",
      "[nltk_data]     Growth\\natural_language_processing\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the third technique is using the nltk library\n",
    "# make sure to install the nltk library first\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 1. Define the NLTK data path absolutely\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "nltk_data_path = os.path.join(script_dir, \"venv\", \"nltk_data\")\n",
    "\n",
    "# 2. Ensure the folder exists\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# 3. Explicitly set the NLTK_DATA environment variable\n",
    "os.environ[\"NLTK_DATA\"] = nltk_data_path\n",
    "\n",
    "# 4. Add the path to NLTK's internal search path list\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, nltk_data_path)\n",
    "\n",
    "print(f\"NLTK Data Path being used: {nltk_data_path}\")\n",
    "print(f\"NLTK data paths (internal): {nltk.data.path}\")\n",
    "\n",
    "\n",
    "# 5. Download the 'punkt' package\n",
    "try:\n",
    "    print(\"Attempting to download 'punkt' package...\")\n",
    "    nltk.download(\"punkt\", download_dir=nltk_data_path, quiet=False)\n",
    "    print(\"'punkt' package download/check complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK punkt download/check: {e}\")\n",
    "    print(\"This might be okay if 'punkt' was already correctly downloaded.\")\n",
    "\n",
    "# NEW: 6. Download the 'punkt_tab' package\n",
    "try:\n",
    "    print(\"\\nAttempting to download 'punkt_tab' package...\")\n",
    "    nltk.download(\"punkt_tab\", download_dir=nltk_data_path, quiet=False)\n",
    "    print(\"'punkt_tab' package download/check complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK punkt_tab download/check: {e}\")\n",
    "    print(\"This might be okay if 'punkt_tab' was already correctly downloaded.\")\n",
    "\n",
    "\n",
    "# 7. Verify that the 'punkt' model file exists at the expected location\n",
    "expected_punkt_path = os.path.join(nltk_data_path, \"tokenizers\", \"punkt\", \"english.pickle\")\n",
    "if not os.path.exists(expected_punkt_path):\n",
    "    print(f\"ERROR: Expected punkt file NOT found at: {expected_punkt_path}\")\n",
    "    print(\"Please check the contents of your venv/nltk_data folder manually.\")\n",
    "    print(\"It should contain 'tokenizers/punkt/english.pickle'.\")\n",
    "else:\n",
    "    print(f\"SUCCESS: punkt file found at: {expected_punkt_path}\")\n",
    "\n",
    "# NEW: Verify that the 'punkt_tab' model file exists\n",
    "expected_punkt_tab_path = os.path.join(nltk_data_path, \"tokenizers\", \"punkt_tab\", \"english.pickle\")\n",
    "if not os.path.exists(expected_punkt_tab_path):\n",
    "    print(f\"ERROR: Expected punkt_tab file NOT found at: {expected_punkt_tab_path}\")\n",
    "    print(\"Please check the contents of your venv/nltk_data folder manually.\")\n",
    "    print(\"It should contain 'tokenizers/punkt_tab/english.pickle'.\")\n",
    "else:\n",
    "    print(f\"SUCCESS: punkt_tab file found at: {expected_punkt_tab_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ab44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokens: ['I', 'have', 'PH.D', 'in', 'Large', 'language', 'models', '.']\n",
      "Sentence Tokens: ['I have PH.D in Large language models.']\n"
     ]
    }
   ],
   "source": [
    "# 8. Perform tokenization\n",
    "#text = \"My interest in natural language processing is growing. My id is humayun.devv@gmail.com\"\n",
    "text = \"I have PH.D in Large language models.\"\n",
    "try:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    print(\"\\nWord Tokens:\", word_tokens)\n",
    "    print(\"Sentence Tokens:\", sent_tokens)\n",
    "except LookupError as e:\n",
    "    print(f\"\\nCaught LookupError during tokenization: {e}\")\n",
    "    print(\"This indicates NLTK still cannot find the required tokenizer.\")\n",
    "    print(\"Double-check the folder structure inside:\")\n",
    "    print(f\"  {nltk_data_path}\")\n",
    "    print(\"It should contain 'tokenizers/punkt/english.pickle' and 'tokenizers/punkt_tab/english.pickle'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy Tokens: ['I', 'have', 'PH.D', 'in', 'Large', 'language', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Spacy library technique\n",
    "# make sure to install the spacy library first\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # python -m spacy download en_core_web_sm\n",
    "text = \"I have PH.D in Large language models.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "# Extract tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"\\nSpacy Tokens:\", tokens)\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "doc_two = nlp(text_two)\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc_two.sents]\n",
    "print(\"Spacy Sentences:\", sentences)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
