{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06579014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing in NLP\n",
    "# author: Muhammad Humayun Khan\n",
    "# The mandatory steps in text preprocessing include:\n",
    "# 1. Lowercasing\n",
    "# 2. Removing HTML tags\n",
    "# 3. Removing URLs  \n",
    "# 4. Removing punctuation\n",
    "# 5. Chat words treatment\n",
    "# 6. Spelling correction\n",
    "# 7. Removing stop words\n",
    "# 8. Handling emojis\n",
    "# 9. Tokenization\n",
    "# 10. Stemming\n",
    "# 11. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae657b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56857ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'datasets/IMDB Dataset.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. lowercase one of the record\n",
    "\n",
    "df['review'][2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443777f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all the records in the review column\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a43dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove the unimportant things such as html tags\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7c45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. the URL links are not important and needs to be removed\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from a string.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_urls)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89f34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Remove the punctuation marks\n",
    "# first check the punctuation list\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d391ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the punctuation marks\n",
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "956fbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a string.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eed999cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String with punctuation  '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"String with punctuation! # @\"\n",
    "\n",
    "text = remove_punctuation(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e4b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production the filming tech...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically theres a family where a little boy j...\n",
       "4        petter matteis love in the time of money is a ...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    im going to have to disagree with the previous...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now want to apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_punctuation)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3728b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Chatword treatment - short words like \"u\" for \"you\", \"r\" for \"are\", etc.\n",
    "\n",
    "chatwords = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"ur\": \"your\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"b4\": \"before\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"pls\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"smh\": \"shaking my head\",   \n",
    "    \"btw\": \"by the way\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"lmao\": \"laughing\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"wbu\": \"what about you\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"idc\": \"I don't care\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"thx\": \"thanks\",    \n",
    "    \"k\": \"okay\",\n",
    "    \"plz\": \"please\",\n",
    "    \"wth\": \"what the hell\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"fml\": \"f*** my life\",\n",
    "    \"tbh\": \"to be honest\"\n",
    "}\n",
    "def chatword_treatment(text):        \n",
    "    for word, replacement in chatwords.items():\n",
    "        text = text.replace(word, replacement)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(chatword_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a0b12ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have a dream that one day this nation will rise up and live out the true meaning of its creed: 'He hold these truths to be self-e\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Spelling correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I havv a dreem that one day this nation will rise up and live out the true meaning of its creed: 'We hold these truths to be self-e\"\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Correct spelling in a string.\"\"\"\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "text = correct_spelling(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8963d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Just\n",
      "[nltk_data]     Bring\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Removing stop words - use the nltk library. Avoid stop words when your task is POS tagging \n",
    "import nltk\n",
    "nltk.download('stopwords')  # Download the stopwords dataset if not already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f567e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one otheare areevieweares mentioned afteare wa...\n",
       "1        wondearefyoul little pareodyouction filming te...\n",
       "2        thoyought wondearefyoul way spend time hot syo...\n",
       "3        basically thearees family whearee little boy j...\n",
       "4        petteare matteis love time money visyoually st...\n",
       "                               ...                        \n",
       "49995    thoyought movie areight good job wasnt careeat...\n",
       "49996    bad plot bad dialogyoue bad acting idiotic dia...\n",
       "49997    catholic tayought paareochial elementaarey sch...\n",
       "49998    im going disagareee pareevioyous comment side ...\n",
       "49999    one expects staare tareeokay movies high aaret...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords   \n",
    "stop_words = set(stopwords.words('english'))   # only english stop words. Others can be added as needed\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from a string.\"\"\"\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "# apply the function to the review column\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffa19b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Programming :red_heart:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Handling emojis - use the emoji library\n",
    "import emoji\n",
    "\n",
    "text = \"I love Programming ❤️\"\n",
    "def handle_emojis(text):\n",
    "    \"\"\"Convert emojis to text.\"\"\"\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "text = handle_emojis(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65106154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', 'and', 'natural', 'language', 'processing.', \"It's\", 'amazing!']\n",
      "[\"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is\", '']\n"
     ]
    }
   ],
   "source": [
    "# 8. Tokenization - \n",
    "# There are different techniques such as using the split function, regular expressions, use the nltk library and spacy library\n",
    "\n",
    "# split function technique\n",
    "text = \"I love programming and natural language processing. It's amazing!\"\n",
    "tokens = text.split()\n",
    "print(tokens)   # word tokenization\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "tokens_two = text_two.split('.')\n",
    "print(tokens_two)   # sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', 'and', 'natural', 'language', 'processing', 'It', 's', 'amazing']\n",
      "[\"Do you love programming and natural language processing? It's amazing\", \" Isn't it? Yes, it is\", '']\n"
     ]
    }
   ],
   "source": [
    "# regular expression technique\n",
    "import re\n",
    "\n",
    "text = \"I love programming and natural language processing. It's amazing!\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)   # word tokenization\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "tokens_two = re.split(r'[.!]', text_two)\n",
    "print(tokens_two)   # sentence tokenization and issue is ? is missing and it is not identified as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749fa837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Path being used: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\n",
      "NLTK data paths (internal): ['C:\\\\Users\\\\Just Bring/nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\share\\\\nltk_data', 'd:\\\\Drive I\\\\Work\\\\Summer 2025 Personal Growth\\\\natural_language_processing\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Just Bring\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n",
      "Attempting to download 'punkt' package...\n",
      "'punkt' package download/check complete.\n",
      "\n",
      "Attempting to download 'punkt_tab' package...\n",
      "'punkt_tab' package download/check complete.\n",
      "SUCCESS: punkt file found at: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\\tokenizers\\punkt\\english.pickle\n",
      "ERROR: Expected punkt_tab file NOT found at: d:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\nltk_data\\tokenizers\\punkt_tab\\english.pickle\n",
      "Please check the contents of your venv/nltk_data folder manually.\n",
      "It should contain 'tokenizers/punkt_tab/english.pickle'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to d:\\Drive I\\Work\\Summer 2025\n",
      "[nltk_data]     Personal\n",
      "[nltk_data]     Growth\\natural_language_processing\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to d:\\Drive I\\Work\\Summer\n",
      "[nltk_data]     2025 Personal\n",
      "[nltk_data]     Growth\\natural_language_processing\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the third technique is using the nltk library\n",
    "# make sure to install the nltk library first\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 1. Define the NLTK data path absolutely\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "nltk_data_path = os.path.join(script_dir, \"venv\", \"nltk_data\")\n",
    "\n",
    "# 2. Ensure the folder exists\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# 3. Explicitly set the NLTK_DATA environment variable\n",
    "os.environ[\"NLTK_DATA\"] = nltk_data_path\n",
    "\n",
    "# 4. Add the path to NLTK's internal search path list\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, nltk_data_path)\n",
    "\n",
    "print(f\"NLTK Data Path being used: {nltk_data_path}\")\n",
    "print(f\"NLTK data paths (internal): {nltk.data.path}\")\n",
    "\n",
    "\n",
    "# 5. Download the 'punkt' package\n",
    "try:\n",
    "    print(\"Attempting to download 'punkt' package...\")\n",
    "    nltk.download(\"punkt\", download_dir=nltk_data_path, quiet=False)\n",
    "    print(\"'punkt' package download/check complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK punkt download/check: {e}\")\n",
    "    print(\"This might be okay if 'punkt' was already correctly downloaded.\")\n",
    "\n",
    "# NEW: 6. Download the 'punkt_tab' package\n",
    "try:\n",
    "    print(\"\\nAttempting to download 'punkt_tab' package...\")\n",
    "    nltk.download(\"punkt_tab\", download_dir=nltk_data_path, quiet=False)\n",
    "    print(\"'punkt_tab' package download/check complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK punkt_tab download/check: {e}\")\n",
    "    print(\"This might be okay if 'punkt_tab' was already correctly downloaded.\")\n",
    "\n",
    "\n",
    "# 7. Verify that the 'punkt' model file exists at the expected location\n",
    "expected_punkt_path = os.path.join(nltk_data_path, \"tokenizers\", \"punkt\", \"english.pickle\")\n",
    "if not os.path.exists(expected_punkt_path):\n",
    "    print(f\"ERROR: Expected punkt file NOT found at: {expected_punkt_path}\")\n",
    "    print(\"Please check the contents of your venv/nltk_data folder manually.\")\n",
    "    print(\"It should contain 'tokenizers/punkt/english.pickle'.\")\n",
    "else:\n",
    "    print(f\"SUCCESS: punkt file found at: {expected_punkt_path}\")\n",
    "\n",
    "# NEW: Verify that the 'punkt_tab' model file exists\n",
    "expected_punkt_tab_path = os.path.join(nltk_data_path, \"tokenizers\", \"punkt_tab\", \"english.pickle\")\n",
    "if not os.path.exists(expected_punkt_tab_path):\n",
    "    print(f\"ERROR: Expected punkt_tab file NOT found at: {expected_punkt_tab_path}\")\n",
    "    print(\"Please check the contents of your venv/nltk_data folder manually.\")\n",
    "    print(\"It should contain 'tokenizers/punkt_tab/english.pickle'.\")\n",
    "else:\n",
    "    print(f\"SUCCESS: punkt_tab file found at: {expected_punkt_tab_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ab44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokens: ['I', 'have', 'PH.D', 'in', 'Large', 'language', 'models', '.']\n",
      "Sentence Tokens: ['I have PH.D in Large language models.']\n"
     ]
    }
   ],
   "source": [
    "# 8. Perform tokenization\n",
    "#text = \"My interest in natural language processing is growing. My id is humayun.devv@gmail.com\"\n",
    "text = \"I have PH.D in Large language models.\"\n",
    "try:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    print(\"\\nWord Tokens:\", word_tokens)\n",
    "    print(\"Sentence Tokens:\", sent_tokens)\n",
    "except LookupError as e:\n",
    "    print(f\"\\nCaught LookupError during tokenization: {e}\")\n",
    "    print(\"This indicates NLTK still cannot find the required tokenizer.\")\n",
    "    print(\"Double-check the folder structure inside:\")\n",
    "    print(f\"  {nltk_data_path}\")\n",
    "    print(\"It should contain 'tokenizers/punkt/english.pickle' and 'tokenizers/punkt_tab/english.pickle'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a72b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy Tokens: ['I', 'have', 'PH.D', 'in', 'Large', 'language', 'models', '.']\n",
      "Spacy Sentences: ['Do you love programming and natural language processing?', \"It's amazing!\", \"Isn't it?\", 'Yes, it is.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Spacy library technique\n",
    "# make sure to install the spacy library first\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # python -m spacy download en_core_web_sm\n",
    "text = \"I have PH.D in Large language models.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "# Extract tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"\\nSpacy Tokens:\", tokens)\n",
    "\n",
    "text_two = \"Do you love programming and natural language processing? It's amazing! Isn't it? Yes, it is.\"\n",
    "doc_two = nlp(text_two)\n",
    "# Extract sentences\n",
    "sentences = [sent.text for sent in doc_two.sents]\n",
    "print(\"Spacy Sentences:\", sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058dd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run run run'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Stemming - # Stemming is the process of reducing a word to its base or root form.\n",
    "# It is often used in information retrieval and natural language processing tasks to reduce words to their base\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_word(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "# Example usage\n",
    "text = \"run runs running\"\n",
    "text_two = \"walk walks walked walking\"\n",
    "stem_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bc255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"probabl the best movi isn't releas yet\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming the text doesn't always brings the expected results as can be seen below in case of movie and probably\n",
    "text = \"Probably the best movie isn't release yet\"\n",
    "stem_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Just\n",
      "[nltk_data]     Bring\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Just\n",
      "[nltk_data]     Bring\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download required resources\u001b[39;00m\n\u001b[0;32m      6\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43momw-1.4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create the lemmatizer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "File \u001b[1;32md:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\lib\\site-packages\\nltk\\downloader.py:774\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    766\u001b[0m     print_to(\n\u001b[0;32m    767\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[0;32m    768\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;66;03m# Error messages\u001b[39;00m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[0;32m    777\u001b[0m         show(msg\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32md:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\lib\\site-packages\\nltk\\downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_package(info, download_dir, force)\n",
      "File \u001b[1;32md:\\Drive I\\Work\\Summer 2025 Personal Growth\\natural_language_processing\\venv\\lib\\site-packages\\nltk\\downloader.py:710\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    708\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, info\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m--> 710\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 16k blocks.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(s)\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 11. Lemmatization - process of reducing a word to its base or dictionary form.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words\n",
    "words = ['running', 'flies', 'better', 'cars', 'geese']\n",
    "\n",
    "# Lemmatize without POS (defaults to noun)\n",
    "print(\"Lemmatizing as nouns (default):\")\n",
    "for word in words:\n",
    "    print(f\"{word} → {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Lemmatize with correct POS\n",
    "print(\"\\nLemmatizing with correct POS:\")\n",
    "print(f\"running (verb) → {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"better (adjective) → {lemmatizer.lemmatize('better', pos='a')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
